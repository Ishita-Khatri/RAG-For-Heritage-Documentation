{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core import (StorageContext, VectorStoreIndex, TreeIndex,\n",
    "                              SimpleDirectoryReader, load_index_from_storage, PromptTemplate)\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "import os\n",
    "import config\n",
    "import Stemmer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = config.key['API_key']\n",
    "llm = OpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_PROMPT = (\n",
    "    \"Context information from multiple sources is given below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"You are a heritage preservation expert. The relevant information is about the material, age, state, deterioration, restauration, etc. of all elements and structures in the Lausanne cathedral.\\n\"\n",
    "    \"Summarize the key points from the given context.\\n\"\n",
    ")\n",
    "\n",
    "INSERT_PROMPT = (\n",
    "    \"The following hierarchy exists: \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"It's about the Lausanne Cathedral and should follow the architectural, structural elements of the building similar to a BIM model: \\n\"\n",
    "    \"{new_info} \\n\"\n",
    "    \"Return the updated structure only.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"database/documents\").load_data()\n",
    "node_parser = SimpleNodeParser.from_defaults(chunk_size=512)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    vector_storage_context = StorageContext.from_defaults(persist_dir=\"database/indexes/vector\")\n",
    "    vector_index = load_index_from_storage(vector_storage_context)\n",
    "    print(\"vector store loaded\")\n",
    "except:\n",
    "    print(\"Can't find persisted vector store, creating new index...\")\n",
    "\n",
    "    # Vector Index\n",
    "    vector_index = VectorStoreIndex(\n",
    "        nodes,\n",
    "        show_progress=True,\n",
    "        summary_prompt = PromptTemplate(SUMMARY_PROMPT),\n",
    "    )    \n",
    "    vector_index.storage_context.persist(persist_dir=\"database/indexes/vector\")\n",
    "    print(\"Vector store created\")\n",
    "\n",
    "try:\n",
    "    bm25_retriever = BM25Retriever.from_persist_dir(\"database/indexes/bm25_retriever\")\n",
    "    print(\"bm25 store created\")\n",
    "except:\n",
    "    print(\"Can't find persisted bm25 store, creating new index...\")\n",
    "    # We can pass in the index, docstore, or list of nodes to create the retriever\n",
    "    bm25_retriever = BM25Retriever.from_defaults(\n",
    "        nodes=nodes,\n",
    "        similarity_top_k=10,\n",
    "        stemmer=Stemmer.Stemmer(\"english\"),\n",
    "        language=\"english\",\n",
    "    )\n",
    "    bm25_retriever.persist(\"database/indexes/bm25_retriever\")\n",
    "    print(\"BM25 store created\")\n",
    "\n",
    "try:\n",
    "    tree_storage_context = StorageContext.from_defaults(persist_dir=\"database/indexes/tree\")\n",
    "    tree_index = load_index_from_storage(tree_storage_context)\n",
    "    print(\"tree store loaded\")\n",
    "except:\n",
    "    print(\"Can't find persisted tree store, creating new index...\")\n",
    "\n",
    "    SUMMARY_PROMPT = PromptTemplate(SUMMARY_PROMPT)\n",
    "    INSERT_PROMPT = PromptTemplate(INSERT_PROMPT)\n",
    "\n",
    "    tree_index = TreeIndex(\n",
    "        nodes,\n",
    "        # storage_context=storage_context,\n",
    "        insert_prompt=INSERT_PROMPT,\n",
    "        summary_prompt=SUMMARY_PROMPT,\n",
    "        show_progress=True,\n",
    "    )\n",
    "    tree_index.storage_context.persist(persist_dir=\"database/indexes/tree\")\n",
    "    print(\"Tree store created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "\n",
    "QUERY_GEN_PROMPT = \"\"\"\\\n",
    "    You are a helpful assistant for a heritage preservation expert that generates multiple search queries based on a \"\n",
    "    single input query.\\\n",
    "    Users are interested in material, age, state, deterioration, restauration, etc. of all elements and structures in the Lausanne cathedral.\\\n",
    "    Unless otherwise specified by the user, it is always helpful to retrieve information at various levels of details, i.e. zoomed out to zoomed in to the specific element requested by the user.\\\n",
    "    Generate {num_queries} detailed search queries, one on each line related to the following input query:\\\n",
    "    Query: {query}\n",
    "    Queries:\n",
    "\"\"\"\n",
    "\n",
    "retriever = QueryFusionRetriever(\n",
    "    [vector_index.as_retriever(verbose=True), bm25_retriever, tree_index.as_retriever(verbose=True)],\n",
    "    similarity_top_k=15,\n",
    "    num_queries=4,\n",
    "    use_async=True,\n",
    "    verbose=True,\n",
    "    query_gen_prompt=PromptTemplate(QUERY_GEN_PROMPT),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(response_mode=\"refine\")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What types and rates of degradation can be expected on the south tower of the transept over the next 50 years?\" + \" give a detailed answer with justifications for heritage preservation experts\"\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What types and rates of degradation can be expected on the south tower of the transept over the next 50 years?\" + \" give a detailed answer with justifications for heritage preservation experts\"\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Where did the Horses of Notre Dame of Lausanne, the bronze statues of four horses, originally come from?\" + \" give a precise, concise and truthful answer without inventing facts for heritage preservation experts\"\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('element_names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_df = pd.DataFrame(df['GivenOrientation GivenName GivenID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(element):\n",
    "    print(element)\n",
    "    query = f\"What information is available concerning the material used for {element}, the date when it was built (current age), \\\n",
    "        current state and deterioration (including types and nature of the deterioration), \\\n",
    "        and restauration efforts (including the types materials used, and which deteriorations were targeted in which time period) for {element}\"\n",
    "    return query_engine.query(query)\n",
    "\n",
    "rag_df['rag response'] = rag_df['GivenOrientation GivenName GivenID'].apply(get_response)\n",
    "rag_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_df.to_csv('results/rag_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_df.to_excel('results/rag_output.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
